{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93cb0ff",
   "metadata": {},
   "source": [
    "# Consuming a Machine Learning Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71667c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38547216",
   "metadata": {},
   "source": [
    "## Understanding the URL\n",
    "\n",
    "\n",
    "### Breaking down the URL\n",
    "\n",
    "After experimenting with the fastAPI's client you may have noticed that we made all requests by pointing to a  specific URL and appending some parameters to it.\n",
    "\n",
    "More concretely:\n",
    "\n",
    "1. Change the local host to the server you have running! \n",
    "2. The endpoint that serves your model is the `/predict` endpoint.\n",
    "\n",
    "Also you can specify the model to use:  'insert model here' (Example: `yolov3` or`yolov3-tiny`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5e6f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://localhost:8080'\n",
    "endpoint = '/predict'\n",
    "model = 'yolov3-tiny'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455a963",
   "metadata": {},
   "source": [
    "To consume your model, you append the endpoint to the base URL to get the full URL. Notice that the parameters are absent for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eba5acef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8080/predict'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_with_endpoint_no_params = base_url + endpoint\n",
    "url_with_endpoint_no_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605584e6",
   "metadata": {},
   "source": [
    "To set any of the expected parameters, the syntax is to add a \"?\" character followed by the name of the parameter and its value.\n",
    "\n",
    "Let's do it and check how the final URL looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ada84906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:8080/predict?model=yolov3-tiny'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_url = url_with_endpoint_no_params + \"?model=\" + model\n",
    "full_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575347e",
   "metadata": {},
   "source": [
    "This endpoint expects both a model's name and an image. But since the image is more complex it is not passed within the URL. Instead we leverage the `requests` library to handle this process.\n",
    "\n",
    "# Sending a request to your server\n",
    "\n",
    "### Coding the response_from_server function\n",
    "\n",
    "As a reminder,  this endpoint expects a POST HTTP request. The `post` function is part of the requests library. \n",
    "\n",
    "To pass the file along with the request, you need to create a dictionary indicating the name of the file ('file' in this case) and the actual file.\n",
    "\n",
    " `status code` is a handy command to check the status of the response the request triggered. **A status code of 200 means that everything went well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eaee6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_from_server(url, image_file, verbose=True):\n",
    "    \"\"\"Makes a POST request to the server and returns the response.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL that the request is sent to.\n",
    "        image_file (_io.BufferedReader): File to upload, should be an image.\n",
    "        verbose (bool): True if the status of the response should be printed. False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        requests.models.Response: Response from the server.\n",
    "    \"\"\"\n",
    "    \n",
    "    files = {'file': image_file}\n",
    "    response = requests.post(url, files=files)\n",
    "    status_code = response.status_code\n",
    "    if verbose:\n",
    "        msg = \"Everything went well!\" if status_code == 200 else \"There was an error when handling the request.\"\n",
    "        print(msg)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a779a2",
   "metadata": {},
   "source": [
    "To test this function, open a file in your filesystem and pass it as a parameter alongside the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10cbfb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error when handling the request.\n"
     ]
    }
   ],
   "source": [
    "with open(\"images/apple.jpg\", \"rb\") as image_file:\n",
    "    prediction = response_from_server(full_url, image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040d6af",
   "metadata": {},
   "source": [
    "Great news! The request was successful. However, you are not getting any information about the objects in the image.\n",
    "\n",
    "To get the image with the bounding boxes and labels,  you need to parse the content of the response into an appropriate format. This process looks very similar to how you read raw images into a cv2 image on the server.\n",
    "\n",
    "To handle this step, let's create a directory called `images_predicted` to save the image to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c897ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"images_predicted\"\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba91555",
   "metadata": {},
   "source": [
    "\n",
    "### Creating the display_image_from_response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63aa808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_response(response):\n",
    "    \"\"\"Display image within server's response.\n",
    "\n",
    "    Args:\n",
    "        response (requests.models.Response): The response from the server after object detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_stream = io.BytesIO(response.content)\n",
    "    image_stream.seek(0)\n",
    "    file_bytes = np.asarray(bytearray(image_stream.read()), dtype=np.uint8)\n",
    "    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "    filename = \"image_with_objects.jpeg\"\n",
    "    cv2.imwrite(f'images_predicted/{filename}', image)\n",
    "    display(Image(f'images_predicted/{filename}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "433ae1d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:1146: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display_image_from_response(prediction)\n",
      "Cell \u001b[1;32mIn[53], line 13\u001b[0m, in \u001b[0;36mdisplay_image_from_response\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimdecode(file_bytes, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_with_objects.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_predicted/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[0;32m     14\u001b[0m display(Image(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_predicted/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:1146: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "display_image_from_response(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b71000",
   "metadata": {},
   "source": [
    "Now you are ready to consume your object detection model through your own client!\n",
    "\n",
    "Let's test it out on some other images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9140c71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:1146: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m image_file:\n\u001b[0;32m      9\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m response_from_server(full_url, image_file, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m display_image_from_response(prediction)\n",
      "Cell \u001b[1;32mIn[53], line 13\u001b[0m, in \u001b[0;36mdisplay_image_from_response\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     11\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimdecode(file_bytes, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_with_objects.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_predicted/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[0;32m     14\u001b[0m display(Image(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_predicted/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:1146: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "image_files = [\n",
    "    'car2.jpg',\n",
    "    'clock3.jpg',\n",
    "    'apples.jpg'\n",
    "]\n",
    "\n",
    "for image_file in image_files:\n",
    "    with open(f\"images/{image_file}\", \"rb\") as image_file:\n",
    "        prediction = response_from_server(full_url, image_file, verbose=False)\n",
    "    \n",
    "    display_image_from_response(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a4bed",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this ungraded lab!** Real life clients and servers have a lot more going on in terms of security and performance. However, the code you just experienced is close to what you see in real production environments. \n",
    "Hopefully, this lab served the purpose of increasing your familiarity with the process of deploying a Deep Learning model, and consuming from it.\n",
    "\n",
    "**Keep it up!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21856eb5",
   "metadata": {},
   "source": [
    "# \n",
    "## Optional Challenge - Adding the confidence level to the request\n",
    "\n",
    "Let's expand on what you have learned so far. The next logical step is to extend the server and the client so that they can accommodate an additional parameter: the level of confidence of the prediction. \n",
    "\n",
    "**To test your extended implementation you must perform the following steps:**\n",
    "\n",
    "- Stop the server by interrupting the Kernel.\n",
    "- Extend the `prediction` function in the server.\n",
    "- Re run the cell containing your server code.\n",
    "- Re launch the server.\n",
    "- Extend your client.\n",
    "- Test it with some images (either with your client or fastAPI's one).\n",
    "\n",
    "Here are some hints that can help you out throughout the process:\n",
    "\n",
    "#### Server side:\n",
    "- The `prediction` function that handles the `/predict` endpoint needs an additional parameter to accept the confidence level. Add this new parameter before the `File` parameter. This is necessary because `File` has a default value and must be specified last.\n",
    "\n",
    "\n",
    "- `cv.detect_common_objects` accepts the `confidence` parameter, which is a floating point number (type `float`in Python).\n",
    "\n",
    "\n",
    "#### Client side:\n",
    "- You can add a new parameter to the URL by extending it with an `&` followed by the name of the parameter and its value. The name of this new parameter must be equal to the name used within the `prediction` function in the server. An example would look like this: `myawesomemodel.com/predict?model=yolov3-tiny&newParam=value` \n",
    "\n",
    "##### Sample Solution:\n",
    "- Once you're done with this optional task or if you got stuck while doing it, you can see a sample solution by one of your course mentors [here](https://community.deeplearning.ai/t/c1-w1-optional-challenge-confidence-level/67619). Just make sure you've already joined our Discourse community as shown in an earlier reading item. This is posted in the [MLEP Learner Projects](https://community.deeplearning.ai/c/machine-learning-engineering-for-production/mlep-learner-projects/224) category and feel free to post your own solution (and other content-related projects) there as well. Just remember **not to post any graded material** so as not to violate the Honor Code. You can instead take one of the tools/concepts taught in the lectures or labs then apply it to a mini-project. [Here](https://community.deeplearning.ai/t/fastapi-for-text-classification-problem-in-arabic/56857) is an example. We encourage you to explore your fellow learners' projects and comment on the ones you find interesting.\n",
    "\n",
    "\n",
    "**You can do it!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
